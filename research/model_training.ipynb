{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\Fertilizer_Pred_MLOPS\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import joblib\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    original_data_path: Path\n",
    "    train_label_path: Path\n",
    "    original_label_path: Path\n",
    "    model_dir: Path\n",
    "    model_name: str\n",
    "    target_column: str\n",
    "    max_depth: int\n",
    "    learning_rate: float\n",
    "    reg_alpha: float\n",
    "    reg_lambda: float\n",
    "    gamma: float\n",
    "    subsample: float\n",
    "    colsample_bytree: float\n",
    "    min_child_weight: int\n",
    "    num_boost_round: int\n",
    "    early_stopping_rounds: int\n",
    "    n_folds: int\n",
    "    random_seed: int\n",
    "    n_jobs: int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Fertilizer_Pred.utils.common import read_yaml, create_directories\n",
    "from src.Fertilizer_Pred.constant import *\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        try:\n",
    "            config = self.config.model_trainer\n",
    "            params = self.params.XGBoost\n",
    "            training = self.params.training\n",
    "            schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "            # Create required directories\n",
    "            create_directories([\n",
    "                Path(config.root_dir),\n",
    "                Path(config.model_dir)\n",
    "            ])\n",
    "\n",
    "             # Verify files exist before proceeding\n",
    "            required_files = [\n",
    "                config.train_data_path,\n",
    "                config.original_data_path,\n",
    "                config.train_label_path,\n",
    "                config.original_label_path\n",
    "            ]\n",
    "\n",
    "            for file_path in required_files:\n",
    "                if not Path(file_path).exists():\n",
    "                    raise FileNotFoundError(f\"Required file not found: {file_path}\")\n",
    "\n",
    "            return ModelTrainerConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                train_data_path=Path(config.train_data_path),\n",
    "                original_data_path=Path(config.original_data_path),\n",
    "                train_label_path=Path(config.train_label_path),\n",
    "                original_label_path=Path(config.original_label_path),\n",
    "                model_dir=Path(config.model_dir),\n",
    "                model_name=config.model_name,\n",
    "                target_column=schema.name,\n",
    "                # XGBoost parameters\n",
    "                max_depth=params.max_depth,\n",
    "                learning_rate=params.learning_rate,\n",
    "                reg_alpha=params.reg_alpha,\n",
    "                reg_lambda=params.reg_lambda,\n",
    "                gamma=params.gamma,\n",
    "                subsample=params.subsample,\n",
    "                colsample_bytree=params.colsample_bytree,\n",
    "                min_child_weight=params.min_child_weight,\n",
    "                #   Training parameters\n",
    "                num_boost_round=training.num_boost_round,\n",
    "                early_stopping_rounds=training.early_stopping_rounds,\n",
    "                n_folds=training.n_folds,\n",
    "                random_seed=training.random_seed,\n",
    "                n_jobs=training.n_jobs\n",
    "             )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Configuration error: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def _load_data(self) -> Tuple[pd.DataFrame, np.ndarray, List[str]]:\n",
    "        \"\"\"Load and validate training data\"\"\"\n",
    "        try:\n",
    "            # Verify files exist\n",
    "            for path in [\n",
    "                self.config.train_data_path,\n",
    "                self.config.original_data_path,\n",
    "                self.config.train_label_path,\n",
    "                self.config.original_label_path\n",
    "            ]:\n",
    "                if not path.exists():\n",
    "                    raise FileNotFoundError(f\"Data file not found: {path}\")\n",
    "\n",
    "            # Load data\n",
    "            df_train = pd.read_csv(self.config.train_data_path)\n",
    "            df_original = pd.read_csv(self.config.original_data_path)\n",
    "            \n",
    "            train_labels = pd.read_csv(self.config.train_label_path)[self.config.target_column]\n",
    "            original_labels = pd.read_csv(self.config.original_label_path)[self.config.target_column]\n",
    "            \n",
    "            # Encode labels\n",
    "            le = LabelEncoder()\n",
    "            y_train = le.fit_transform(train_labels)\n",
    "            y_original = le.transform(original_labels)\n",
    "            \n",
    "            # Combine data\n",
    "            X = pd.concat([df_train, df_original], axis=0)\n",
    "            y = np.concatenate([y_train, y_original])\n",
    "            \n",
    "            return X, y, le.classes_\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Data loading failed: {str(e)}\") from e\n",
    "\n",
    "    def _train_model(self, X: pd.DataFrame, y: np.ndarray, classes: List[str]):\n",
    "        \"\"\"Train XGBoost model with configured parameters\"\"\"\n",
    "        try:\n",
    "            # Convert categorical columns to codes\n",
    "            cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "            for col in cat_cols:\n",
    "                X[col] = X[col].astype('category').cat.codes\n",
    "\n",
    "            params = {\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': len(classes),\n",
    "                'max_depth': self.config.max_depth,\n",
    "                'learning_rate': self.config.learning_rate,\n",
    "                'reg_alpha': self.config.reg_alpha,\n",
    "                'reg_lambda': self.config.reg_lambda,\n",
    "                'gamma': self.config.gamma,\n",
    "                'subsample': self.config.subsample,\n",
    "                'colsample_bytree': self.config.colsample_bytree,\n",
    "                'min_child_weight': self.config.min_child_weight,\n",
    "                'random_state': self.config.random_seed,\n",
    "                'n_jobs': self.config.n_jobs,\n",
    "                'tree_method': 'hist',\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'enable_categorical': False  # We've already converted categories\n",
    "            }\n",
    "\n",
    "            skf = StratifiedKFold(n_splits=self.config.n_folds,\n",
    "                                shuffle=True,\n",
    "                                random_state=self.config.random_seed)\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "                print(f\"\\nTraining Fold {fold+1}\")\n",
    "                \n",
    "                X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "                X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "                # Create DMatrix (no need for enable_categorical=True since we converted)\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "                model = xgb.train(\n",
    "                    params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=self.config.num_boost_round,\n",
    "                    early_stopping_rounds=self.config.early_stopping_rounds,\n",
    "                    evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "                    verbose_eval=200\n",
    "                )\n",
    "                \n",
    "                model_path = os.path.join(self.config.model_dir, f\"xgb_fold{fold}.bin\")\n",
    "                model.save_model(model_path)\n",
    "                print(f\"Saved model for fold {fold+1} to {model_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during model training: {str(e)}\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Execute full training pipeline\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.config.model_dir, exist_ok=True)\n",
    "            X, y, classes = self._load_data()\n",
    "            print(f\"Starting training with {len(classes)} fertilizer classes\")\n",
    "            self._train_model(X, y, classes)\n",
    "            print(\"Training completed successfully!\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Training pipeline failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 00:29:30,048: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-10 00:29:30,053: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-10 00:29:30,064: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-07-10 00:29:30,067: INFO: common: created directory at: artifacts]\n",
      "[2025-07-10 00:29:30,069: INFO: common: created directory at: artifacts\\model_trainer]\n",
      "[2025-07-10 00:29:30,071: INFO: common: created directory at: artifacts\\model_trainer\\models]\n",
      "Starting training with 7 fertilizer classes\n",
      "\n",
      "Training Fold 1\n",
      "[0]\ttrain-mlogloss:1.94567\tval-mlogloss:1.94571\n",
      "[200]\ttrain-mlogloss:1.92462\tval-mlogloss:1.93156\n",
      "[400]\ttrain-mlogloss:1.91306\tval-mlogloss:1.92611\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    trainer_config = config.get_model_trainer_config()\n",
    "    trainer = ModelTrainer(config=trainer_config)\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels columns:\n",
      "['target']\n",
      "Original labels columns:\n",
      "['target']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Train labels columns:\")\n",
    "print(pd.read_csv(\"artifacts/data_transformation/train_labels.csv\").columns.tolist())\n",
    "\n",
    "print(\"Original labels columns:\")\n",
    "print(pd.read_csv(\"artifacts/data_transformation/original_labels.csv\").columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
